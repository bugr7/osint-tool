# osint_tool_final.py
import requests
import json
import csv
import time
from urllib.parse import quote

# ===== Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ù…Ù†ØµØ§Øª =====
PLATFORMS = {
    "YOUTUBE": "site:youtube.com",
    "TIKTOK": "site:tiktok.com",
    "REDDIT": "site:reddit.com",
    "LINKEDIN": "site:linkedin.com",
    "FACEBOOK": "site:facebook.com",
    "INSTAGRAM": "site:instagram.com"
}

# ===== Ø¯ÙˆØ§Ù„ Ø§Ù„Ø¨Ø­Ø« =====
def search_bing(query, limit=10):
    """Ø¨Ø­Ø« ÙÙŠ Bing"""
    url = f"https://www.bing.com/search?q={quote(query)}"
    headers = {"User-Agent": "Mozilla/5.0"}
    results = []
    try:
        r = requests.get(url, headers=headers, timeout=10)
        if r.status_code == 200:
            links = [line.split('"')[0] for line in r.text.split("href=\"")[1:]]
            for link in links:
                if link.startswith("http") and not link.startswith("https://go.microsoft.com"):
                    results.append(link)
                if len(results) >= limit:
                    break
    except Exception as e:
        print(f"[!] Ø®Ø·Ø£ ÙÙŠ Bing: {e}")
    return results

def search_duckduckgo(query, limit=10):
    """Ø¨Ø­Ø« ÙÙŠ DuckDuckGo"""
    url = f"https://duckduckgo.com/html/?q={quote(query)}"
    headers = {"User-Agent": "Mozilla/5.0"}
    results = []
    try:
        r = requests.get(url, headers=headers, timeout=10)
        if r.status_code == 200:
            links = [line.split('"')[0] for line in r.text.split("uddg=")[1:]]
            for link in links:
                if link.startswith("http"):
                    results.append(link)
                if len(results) >= limit:
                    break
    except Exception as e:
        print(f"[!] Ø®Ø·Ø£ ÙÙŠ DuckDuckGo: {e}")
    return results

def clean_links(links, domain):
    """ÙÙ„ØªØ±Ø© Ø§Ù„Ø±ÙˆØ§Ø¨Ø· Ù„ØªØ®Øµ Ø§Ù„Ù…Ù†ØµØ© ÙÙ‚Ø·"""
    return [l for l in links if domain in l]

# ===== Ø§Ù„Ø¨Ø±Ù†Ø§Ù…Ø¬ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ =====
def main():
    query = input("[?] Ø£Ø¯Ø®Ù„ Ø§Ø³Ù… Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ø£Ùˆ Ø§Ù„Ø§Ø³Ù… Ø§Ù„ÙƒØ§Ù…Ù„: ").strip()
    all_results = {}

    for platform, site in PLATFORMS.items():
        print(f"\nğŸ” Ø¬Ø§Ø±Ù Ø§Ù„Ø¨Ø­Ø« ÙÙŠ {platform} ...")
        q = f"{query} {site}"

        # Ø¬Ø±Ø¨ Bing Ø£ÙˆÙ„Ø§Ù‹
        results = search_bing(q, limit=15)

        # Ù„Ùˆ ÙØ§Ø±ØºØ© Ø¬Ø±Ø¨ DuckDuckGo
        if not results:
            results = search_duckduckgo(q, limit=15)

        # ÙÙ„ØªØ±Ø© Ø§Ù„Ø±ÙˆØ§Ø¨Ø·
        results = clean_links(results, site.replace("site:", ""))[:10]

        if results:
            for link in results:
                print(f"ğŸ‘‰ {link}")
        else:
            print("âŒ Ù„Ø§ ØªÙˆØ¬Ø¯ Ù†ØªØ§Ø¦Ø¬.")

        all_results[platform] = results

        time.sleep(2)  # ØªØ£Ø®ÙŠØ± Ø¨Ø³ÙŠØ· Ù„ØªÙØ§Ø¯ÙŠ Ø§Ù„Ø¨Ù„ÙˆÙƒ

    # ===== Ø­ÙØ¸ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ =====
    with open("results.txt", "w", encoding="utf-8") as f:
        for platform, links in all_results.items():
            f.write(f"\n{platform}:\n")
            for link in links:
                f.write(link + "\n")

    with open("results.csv", "w", newline="", encoding="utf-8") as f:
        writer = csv.writer(f)
        writer.writerow(["Platform", "Link"])
        for platform, links in all_results.items():
            for link in links:
                writer.writerow([platform, link])

    with open("results.json", "w", encoding="utf-8") as f:
        json.dump(all_results, f, indent=4, ensure_ascii=False)

    print("\nâœ… Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø­ÙØ¸Øª ÙÙŠ results.txt Ùˆ results.csv Ùˆ results.json")

if __name__ == "__main__":
    main()
