# tool.py
import os
import platform
import requests
from bs4 import BeautifulSoup
import urllib.parse
import time
import random

# ÙŠÙ…ÙƒÙ† ØªØ­Ø¯ÙŠØ¯ Ø±Ø§Ø¨Ø· Ø§Ù„Ø³ÙŠØ±ÙØ± Ù…Ù† Ù…ØªØºÙŠØ± Ø¨ÙŠØ¦Ø© SERVER_URL Ø£Ùˆ ÙŠØ³ØªØ®Ø¯Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠ
SERVER_URL = os.getenv("SERVER_URL", "https://osint-tool-production.up.railway.app/log_search")

PLATFORMS = {
    "Facebook": "facebook.com",
    "Instagram": "instagram.com",
    "Youtube": "youtube.com",
    "TikTok": "tiktok.com",
    "Snapchat": "snapchat.com",
    "Reddit": "reddit.com",
    "Twitter": "twitter.com",
    "Pinterest": "pinterest.com",
    "LinkedIn": "linkedin.com",
}

REQUEST_DELAY = float(os.getenv("REQUEST_DELAY", 1.5))
MAX_RESULTS = int(os.getenv("MAX_RESULTS", 10))
MAX_RETRIES = int(os.getenv("MAX_RETRIES", 6))  # Ù…Ø­Ø§ÙˆÙ„Ø© Ø£ÙƒØ«Ø± Ù‚Ø¨Ù„ Ø§Ù„Ø§Ø³ØªØ³Ù„Ø§Ù…
TIMEOUT = float(os.getenv("REQUEST_TIMEOUT", 25.0))

# Ù‚Ø§Ø¦Ù…Ø© User-Agents Ù„Ù„ØªØ¯ÙˆÙŠØ± Ù„ØªÙ‚Ù„ÙŠÙ„ Ø§Ù„Ø­Ø¬Ø¨
USER_AGENTS = [
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 13_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36",
    "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Mozilla/5.0 (iPhone; CPU iPhone OS 17_0 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.0 Mobile/15E148 Safari/604.1",
]

session = requests.Session()


def _set_random_ua():
    ua = random.choice(USER_AGENTS)
    session.headers.update({"User-Agent": ua})


def log_user_search(search_text):
    """Ø¥Ø±Ø³Ø§Ù„ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø¨Ø­Ø« Ø¥Ù„Ù‰ Ø§Ù„Ø³ÙŠØ±ÙØ± (Railway -> Turso)."""
    try:
        ip = requests.get("https://api64.ipify.org?format=json", timeout=8).json().get("ip", "0.0.0.0")
    except Exception:
        ip = "0.0.0.0"

    data = {
        "username": platform.node(),
        "os": platform.system() + " " + platform.release(),
        "country": "Unknown",
        "ip": ip,
        "search": search_text
    }

    try:
        # Ø¥Ø±Ø³Ø§Ù„ Ø¨Ø¯ÙˆÙ† Ø§Ù†ØªØ¸Ø§Ø± Ù†ØªÙŠØ¬Ø© ÙƒØ¨ÙŠØ±Ø© (timeout Ù‚ØµÙŠØ±)
        requests.post(SERVER_URL, json=data, timeout=10)
    except Exception as e:
        # Ù„Ø§ Ù†ÙˆÙ‚Ù Ø§Ù„Ø¨Ø­Ø« Ø¥Ø°Ø§ ÙØ´Ù„ Ø§Ù„ØªØ³Ø¬ÙŠÙ„
        print("âš ï¸ Failed to log user search:", e)


def _extract_links_from_ddg_html(resp_text):
    """Ù…Ø­Ø§ÙˆÙ„Ø© Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø±ÙˆØ§Ø¨Ø· Ù…Ù† HTML Ø§Ù„Ø®Ø§Øµ Ø¨Ù€ DuckDuckGo."""
    soup = BeautifulSoup(resp_text, "html.parser")
    links = []

    # Ø£ÙØ¶Ù„ Ø§Ø®ØªÙŠØ§Ø± Ù„Ø¹Ù†Ø§ØµØ± Ø§Ù„Ù†ØªØ§Ø¦Ø¬
    anchors = soup.select("a.result__a")
    if not anchors:
        anchors = soup.find_all("a")

    for a in anchors:
        href = a.get("href") or ""
        link = None
        if href:
            # Ø­Ø§Ù„Ø© uddg (DuckDuckGo ÙŠØ´ÙØ± Ø§Ù„Ø±Ø§Ø¨Ø·)
            if "uddg=" in href:
                # Ù†Ø­Ø§ÙˆÙ„ ÙÙƒ Ù‚ÙŠÙ…Ø© Ø§Ù„uddg
                try:
                    q = urllib.parse.urlparse(href).query
                    params = urllib.parse.parse_qs(q)
                    if "uddg" in params:
                        link = urllib.parse.unquote(params["uddg"][0])
                except Exception:
                    link = None
            else:
                link = href

        # Ø£Ø­ÙŠØ§Ù†Ø§ Ø§Ù„Ø±Ø§Ø¨Ø· Ù…Ø­ÙÙˆØ¸ ÙÙŠ data-href Ø£Ùˆ data-redirect
        if not link:
            data_href = a.get("data-href") or a.get("data-redirect")
            if data_href:
                link = data_href

        if link and link.startswith("http") and "duckduckgo" not in link:
            if link not in links:
                links.append(link)

    # ÙØ§Ù„Ø¨Ø§Ùƒ Ø¨Ø³ÙŠØ·: regex href
    if not links:
        import re
        found = re.findall(r'href="(https?://[^"]+)"', resp_text)
        for link in found:
            if "duckduckgo" not in link and link not in links:
                links.append(link)

    return links


def duckduckgo_search(query, site=None, max_results=MAX_RESULTS):
    """Ø­Ø§ÙˆÙ„ Ø¬Ù„Ø¨ Ù†ØªØ§Ø¦Ø¬ Ù…Ù† DuckDuckGo HTML endpoint Ù…Ø¹ retries Ùˆbackoff.
       Ø¥Ø°Ø§ Ù†Ø¬Ø­ ÙŠØ¹ÙŠØ¯ Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ø±ÙˆØ§Ø¨Ø· (Ø­ØªÙ‰ max_results)."""
    search_query = f"{query} site:{site}" if site else query
    url = "https://html.duckduckgo.com/html/"
    params = {"q": search_query}
    links = []

    for attempt in range(MAX_RETRIES):
        _set_random_ua()
        try:
            resp = session.get(url, params=params, timeout=TIMEOUT)
            status = getattr(resp, "status_code", None)
            if status == 200:
                candidate = _extract_links_from_ddg_html(resp.text)
                for l in candidate:
                    if l not in links:
                        links.append(l)
                    if len(links) >= max_results:
                        break
                if links:
                    return links[:max_results]
                # Ù„Ùˆ Ù„Ù… Ù†Ø¬Ø¯ Ø´ÙŠØ¦Ø§ØŒ Ù†Ø¬Ø±Ø¨ Ù…Ø­Ø§ÙˆÙ„Ø§Øª Ø£Ø®Ø±Ù‰
            elif status in (202, 429):
                wait = (attempt + 1) * 3
                print(f"âš ï¸ DuckDuckGo {status}, retrying in {wait}s (attempt {attempt + 1})")
                time.sleep(wait)
                continue
            else:
                print(f"âŒ DuckDuckGo returned status: {status}")
                # Ø­Ø§Ù„Ø§Øª Ø£Ø®Ø±Ù‰ Ù†Ø®Ø±Ø¬ ÙˆÙ†Ø¬Ø±Ø¨ fallback
                break
        except Exception as e:
            print("âš ï¸ DuckDuckGo request error:", e)
            time.sleep(2)

    return links  # Ù‚Ø¯ ØªÙƒÙˆÙ† ÙØ§Ø±ØºØ© -> fallback ÙŠØ­Ø¯Ø« Ø®Ø§Ø±Ø¬Ø§Ù‹


def _extract_links_from_bing_html(resp_text):
    soup = BeautifulSoup(resp_text, "html.parser")
    links = []

    # Bing: Ø¹Ù†Ø§ØµØ± Ø§Ù„Ù†ØªØ§Ø¦Ø¬ ØºØ§Ù„Ø¨Ø§ Ø¯Ø§Ø®Ù„ li.b_algo a  Ø£Ùˆ h2 > a
    results = soup.select("li.b_algo h2 a")
    if not results:
        results = soup.select("li.b_algo a")

    for a in results:
        href = a.get("href")
        if href and href.startswith("http"):
            if href not in links:
                links.append(href)

    # ÙØ§Ù„Ø¨Ø§Ùƒ Ø¨Ø³ÙŠØ·
    if not links:
        import re
        found = re.findall(r'href="(https?://[^"]+)"', resp_text)
        for link in found:
            if link not in links:
                links.append(link)

    return links


def bing_search(query, site=None, max_results=MAX_RESULTS):
    """Fallback: scrape Bing search results page."""
    search_query = f"{query} site:{site}" if site else query
    url = "https://www.bing.com/search"
    params = {"q": search_query, "count": str(max_results * 2)}
    links = []

    for attempt in range(3):
        _set_random_ua()
        try:
            resp = session.get(url, params=params, timeout=TIMEOUT)
            if resp.status_code == 200:
                candidate = _extract_links_from_bing_html(resp.text)
                for l in candidate:
                    if l not in links:
                        links.append(l)
                    if len(links) >= max_results:
                        break
                return links[:max_results]
            else:
                time.sleep(1 + attempt)
        except Exception as e:
            print("âš ï¸ Bing request error:", e)
            time.sleep(1)

    return links


def search_identifier(identifier):
    results_total = []

    for platform_name, domain in PLATFORMS.items():
        print(f"ğŸ” Searching {platform_name}...")
        links = duckduckgo_search(identifier, site=domain, max_results=MAX_RESULTS)
        # Ù„Ùˆ DuckDuckGo Ø£Ø¹Ø·Ù‰ Ù†ØªØ§Ø¦Ø¬ Ù‚Ù„ÙŠÙ„Ø© Ø£Ùˆ ØµÙØ±ØŒ Ù†Ø³ØªØ®Ø¯Ù… Bing ÙƒÙ€ fallback
        if not links or len(links) < MAX_RESULTS:
            # Ù†Ø¬Ø±Ø¨ Ø²ÙŠØ§Ø¯Ø© Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø§Øª Ù„Ù€ DuckDuckGo Ø³Ø±ÙŠØ¹Ø§ Ù…Ø±Ø© Ø£Ø®Ø±Ù‰ (Ù…Ø­Ø§ÙˆÙ„Ø© Ø£Ø®ÙŠØ±Ø©)
            extra = duckduckgo_search(identifier, site=domain, max_results=MAX_RESULTS)
            for l in extra:
                if l not in links:
                    links.append(l)
                    if len(links) >= MAX_RESULTS:
                        break

        if not links:
            print(f"âš ï¸ No/insufficient from DuckDuckGo for {platform_name}. Using Bing fallback...")
            links = bing_search(identifier, site=domain, max_results=MAX_RESULTS)

        count = len(links)
        print(f"âœ… {platform_name}: {count}/{MAX_RESULTS}" if count else f"âŒ {platform_name}: 0/{MAX_RESULTS}")
        for link in links[:MAX_RESULTS]:
            print(f"   {link}")
            results_total.append({"platform": platform_name, "link": link})

        # ØªØ£Ø®ÙŠØ± Ø¨Ø³ÙŠØ· Ù„ØªØ®ÙÙŠÙ Ø§Ù„Ø­Ø¬Ø¨
        time.sleep(REQUEST_DELAY + random.random() * 0.5)

    return results_total


def main():
    print("OSINT tool - local scraping mode (DDG primary, Bing fallback)")
    while True:
        identifier = input("[?] Enter username or first/last name: ").strip()
        if not identifier:
            print("âŒ No input provided.")
            continue

        # Log to server (Railway -> Turso) - Ù„Ø§ Ù†Ø±Ø³Ù„ Ù†ØªØ§Ø¦Ø¬ DuckDuckGo Ø¥Ù„Ù‰ Ø§Ù„Ø³ÙŠØ±ÙØ±
        log_user_search(identifier)

        results = search_identifier(identifier)
        total = len(results)
        print(f"\nğŸ” Total links collected: {total}\n")

        again = input("[?] Do you want to search again? (yes/no): ").strip().lower()
        if again not in ("yes", "y"):
            print("âœ” Exiting.")
            break


if __name__ == "__main__":
    main()
