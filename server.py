# server.py
from flask import Flask, request, jsonify
from libsql_client import create_client_sync
from migrate import migrate
import os
import time
import requests
from bs4 import BeautifulSoup
import traceback
import re
import urllib.parse
import random

app = Flask(__name__)

# ===== Ø¥Ø¹Ø¯Ø§Ø¯ Turso (Ù…Ù† Ø§Ù„Ù…ØªØºÙŠØ±Ø§Øª Ø§Ù„Ø¨ÙŠØ¦ÙŠØ©) =====
DATABASE_URL = os.getenv("DATABASE_URL")
AUTH_TOKEN = os.getenv("AUTH_TOKEN")

client = None
try:
    if DATABASE_URL and AUTH_TOKEN:
        client = create_client_sync(url=DATABASE_URL, auth_token=AUTH_TOKEN)
        migrate(client)
    else:
        print("âš ï¸ DATABASE_URL or AUTH_TOKEN not set. DB disabled.")
except Exception as e:
    print("âš ï¸ Turso init failed:", e)

# Ø¥Ù†Ø´Ù€Ø§Ø¡ Ø§Ù„Ø¬Ø¯Ø§ÙˆÙ„ Ù„Ùˆ Ø£Ù…ÙƒÙ†
if client:
    try:
        client.execute("""
        CREATE TABLE IF NOT EXISTS users_log (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            username TEXT,
            os TEXT,
            country TEXT,
            ip TEXT,
            search TEXT,
            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
        )
        """)
    except Exception as e:
        print("Error creating users_log:", e)

    try:
        client.execute("""
        CREATE TABLE IF NOT EXISTS search_cache (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            query TEXT,
            platform TEXT,
            link TEXT,
            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
        )
        """)
    except Exception as e:
        print("Error creating search_cache:", e)


# ===== Ø§Ù„Ù…Ù†ØµØ§Øª =====
PLATFORMS = {
    "Facebook": "facebook.com",
    "Instagram": "instagram.com",
    "Youtube": "youtube.com",
    "TikTok": "tiktok.com",
    "Snapchat": "snapchat.com",
    "Reddit": "reddit.com",
    "Twitter": "twitter.com",
    "Pinterest": "pinterest.com",
    "LinkedIn": "linkedin.com",
}

# Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ø´Ø¨ÙƒØ© / Ø§Ù„Ø³Ù„ÙˆÙƒ
REQUEST_DELAY = float(os.getenv("REQUEST_DELAY", 1.2))  # ØªØ£Ø®ÙŠØ± Ø§ÙØªØ±Ø§Ø¶ÙŠ Ù„ØªÙ‚Ù„ÙŠÙ„ Ø§Ù„Ø­Ø¸Ø±
MAX_RETRIES = int(os.getenv("MAX_RETRIES", 2))
DESIRED_PER_PLATFORM = int(os.getenv("DESIRED_PER_PLATFORM", 10))

# Ø¬Ù„Ø³Ø© requests Ù…Ø¹ Header Ø«Ø§Ø¨Øª
session = requests.Session()
HEADERS_POOL = [
    # Ù…Ø¬Ù…ÙˆØ¹Ø© User-Agents (ÙŠÙ…ÙƒÙ† Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ù…Ø²ÙŠØ¯)
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/16.0 Safari/605.1.15",
    "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/115.0.0.0 Safari/537.36",
]
session.headers.update({"User-Agent": random.choice(HEADERS_POOL)})


def generate_permutations(name):
    """
    ØªÙˆÙ„ÙŠØ¯ ØµÙŠØº Ù…ØªØ¹Ø¯Ø¯Ø© Ù…Ù† Ø§Ù„Ø§Ø³Ù… Ù„Ø²ÙŠØ§Ø¯Ø© Ø§Ù„Ø§Ø­ØªÙ…Ø§Ù„Ø§Øª:
    - Ø§Ù„Ø§Ø³Ù… ÙƒÙ…Ø§ Ù‡Ùˆ
    - Ø¨Ø¯ÙˆÙ† Ù…Ø³Ø§ÙØ§Øª
    - Ø¨Ù†Ù‚Ø·Ø©/Ø´Ø±Ø·Ø©/underscore
    - firstlast Ùˆ lastfirst
    - Ø¥Ø¶Ø§ÙØ© Ø§Ø­ØªÙ…Ø§Ù„Ø§Øª Ù…Ø¹ Ø­Ø°Ù Ø£Ø­Ø±Ù Ù…ØªÙˆØ³Ø·Ø©
    """
    name = name.strip()
    parts = [p for p in name.split() if p]
    perms = []
    # Ø§Ù„Ø£ØµÙ„ÙŠØ©
    perms.append(name)
    # Ø¨Ø¯ÙˆÙ† Ù…Ø³Ø§ÙØ§Øª
    perms.append("".join(parts))
    # Ù†Ù‚Ø· Ùˆ underscores Ùˆ Ø´Ø±Ø·Ø§Øª
    if len(parts) >= 2:
        perms.append(".".join(parts))
        perms.append("_".join(parts))
        perms.append("-".join(parts))
        perms.append(parts[0] + parts[-1])
        perms.append(parts[-1] + parts[0])
    # ÙƒÙ„ Ø¬Ø²Ø¡ Ù„ÙˆØ­Ø¯Ù‡
    for p in parts:
        perms.append(p)
    # Ø¥Ø¶Ø§ÙØ§Øª: lowercase
    perms = list(dict.fromkeys([p.lower() for p in perms if p]))  # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ù…ÙƒØ±Ø±Ø§Øª ÙˆØ§Ù„Ù…Ø­Ø§ÙØ¸Ø© Ø¹Ù„Ù‰ Ø§Ù„ØªØ±ØªÙŠØ¨
    return perms


def extract_links_from_html(resp_text, max_results):
    links = []
    # Ø¨Ø§Ùƒ Ø£Ø¨ Ø¹Ø¨Ø± regex
    found = re.findall(r'href=["\'](https?://[^"\' >]+)', resp_text)
    for l in found:
        if 'duckduckgo.com' not in l and l not in links:
            links.append(l)
        if len(links) >= max_results:
            break
    return links


def duckduckgo_search_links(query, site=None, num_results=10):
    """
    Ø¨Ø­Ø« Ø°ÙƒÙŠ: Ù†Ø­Ø§ÙˆÙ„ Ø¹Ø¯Ø© permutations Ù…Ù† query Ø­ØªÙ‰ Ù†Ù…Ù„Ø£ num_results Ø¥Ù† Ø£Ù…ÙƒÙ†.
    Ù†Ø³ØªØ®Ø¯Ù… DuckDuckGo HTML endpoint ÙˆÙ†ÙÙƒ Ø±ÙˆØ§Ø¨Ø· uddg Ø¥Ø°Ø§ Ù„Ø²Ù….
    """
    base_query = f"{query} site:{site}" if site else query
    collected = []
    tried_queries = set()
    # Ø§Ø¬Ù…Ø¹ permutations Ù„ØªØ¬Ø±Ø¨Ø© Ù…ØªØªØ§Ø¨Ø¹Ø©
    name_perms = generate_permutations(query)
    # Ø£Ø¶Ù Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù… Ø§Ù„ÙƒØ§Ù…Ù„ ÙÙŠ Ø§Ù„Ø¨Ø¯Ø§ÙŠØ©
    candidate_queries = [base_query] + [f"{p} site:{site}" if site else p for p in name_perms]

    # Ø­Ø¯ Ø£Ù‚ØµÙ‰ Ù„Ù„Ù€ attempts
    for candidate in candidate_queries:
        if len(collected) >= num_results:
            break
        if candidate in tried_queries:
            continue
        tried_queries.add(candidate)

        # ØªØºÙŠÙŠØ± user-agent Ø¹Ø´ÙˆØ§Ø¦ÙŠØ§Ù‹ Ù…Ù† Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø©
        session.headers.update({"User-Agent": random.choice(HEADERS_POOL)})

        url = "https://html.duckduckgo.com/html/"
        params = {"q": candidate}

        for attempt in range(MAX_RETRIES + 1):
            try:
                resp = session.get(url, params=params, timeout=20)
                print(f"ğŸ” DuckDuckGo search '{candidate[:80]}' | status={resp.status_code}", flush=True)

                if resp.status_code == 200:
                    soup = BeautifulSoup(resp.text, "html.parser")
                    anchors = soup.select("a.result__a")
                    if not anchors:
                        anchors = soup.find_all("a")

                    for a in anchors:
                        link = None
                        href = a.get("href")
                        if href:
                            # ÙÙƒ udgg param
                            if "uddg=" in href:
                                m = re.search(r"uddg=([^&]+)", href)
                                if m:
                                    try:
                                        link = urllib.parse.unquote(m.group(1))
                                    except Exception:
                                        link = None
                            else:
                                link = href

                        if not link:
                            data_href = a.get("data-href") or a.get("data-redirect")
                            if data_href:
                                link = data_href

                        if link and link.startswith("http"):
                            if "duckduckgo.com" not in link and link not in collected:
                                # simple filter: ÙŠØ¬Ø¨ Ø£Ù† ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ø§Ù„Ù€ domain Ø¥Ø°Ø§ ÙƒØ§Ù† Ù…Ø·Ø¨ÙˆØ¹Ø§Ù‹ ÙÙŠ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù… (site:)
                                if site and site not in link:
                                    # Ø¥Ø°Ø§ Ø§Ù„Ø¨Ø­Ø« ÙƒØ§Ù† site:domainØŒ ØªØ¬Ø§Ù‡Ù„ Ø§Ù„Ø±ÙˆØ§Ø¨Ø· Ø§Ù„ØªÙŠ Ù„Ø§ ØªÙ…Ù„Ùƒ Ø§Ù„Ø¯ÙˆÙ…ÙŠÙ†
                                    continue
                                collected.append(link)
                                print(f"   + found: {link}", flush=True)
                        if len(collected) >= num_results:
                            break

                    # Ù„Ùˆ Ù„Ù… Ù†Ø­ØµÙ„ Ø¹Ù„Ù‰ Ø´ÙŠØ¡ ÙƒØ§ÙÙ: Ø§Ø³ØªØ®Ø¯Ù… regex Ø¨Ø§Ùƒ Ø£Ø¨
                    if len(collected) < num_results:
                        extra = extract_links_from_html(resp.text, num_results - len(collected))
                        for l in extra:
                            if l not in collected:
                                if site and site not in l:
                                    continue
                                collected.append(l)
                                if len(collected) >= num_results:
                                    break

                    break  # Ø®Ø±ÙˆØ¬ Ù…Ù† attempts Ø¹Ù†Ø¯ 200

                elif resp.status_code in (429, 202):
                    wait = 2 ** attempt
                    print(f"â³ Got {resp.status_code}, backoff {wait}s (attempt {attempt + 1})", flush=True)
                    time.sleep(wait)
                    continue
                else:
                    print("âŒ DuckDuckGo returned status:", resp.status_code, flush=True)
                    break

            except Exception as e:
                print("âš ï¸ Error searching:", e, flush=True)
                traceback.print_exc()
                time.sleep(1)
                continue

        # Ù‚Ù„ÙŠÙ„ Ù…Ù† Ø§Ù„ØªØ£Ø®ÙŠØ± Ø¨ÙŠÙ† Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª
        time.sleep(0.5)

    # ØªØ£ÙƒØ¯ Ø£Ù†Ù†Ø§ Ù†Ø¹ÙŠØ¯ Ø­ØªÙ‰ Ù„Ùˆ ÙƒØ§Ù†Øª ÙØ§Ø±ØºØ©
    return collected[:num_results]


@app.route("/search", methods=["POST"])
def search():
    try:
        data = request.get_json(silent=True) or {}
        identifier = (data.get("identifier", "") or "").strip()
        print(f"ğŸ” Searching for: {identifier}", flush=True)

        if not identifier:
            return jsonify([])

        # ØªØ®Ø²ÙŠÙ† Ø§Ù„Ø¨Ø­Ø«
        if client:
            try:
                client.execute(
                    "INSERT INTO users_log (username, os, country, ip, search) VALUES (?, ?, ?, ?, ?)",
                    ("server", "server", "Unknown", "0.0.0.0", identifier)
                )
            except Exception as e:
                print("DB insert error:", e)

        results = []

        for platform_name, domain in PLATFORMS.items():
            try:
                # Ø¬Ù„Ø¨ Ù…Ù† Ø§Ù„ÙƒØ§Ø´ Ø£ÙˆÙ„Ø§Ù‹
                cached = []
                if client:
                    try:
                        res = client.execute(
                            "SELECT link FROM search_cache WHERE query=? AND platform=?",
                            (identifier, platform_name)
                        )
                        if hasattr(res, "fetchall"):
                            cached = res.fetchall()
                        else:
                            cached = list(res)
                    except Exception as e:
                        print("Cache select error:", e)
                        cached = []

                if cached:
                    for row in cached:
                        link = row[0] if isinstance(row, (list, tuple)) else row
                        results.append({"platform": platform_name, "link": link})
                    print(f"ğŸ—„ï¸  Loaded {len(cached)} cached for {platform_name}", flush=True)
                    continue

                # Ø¨Ø­Ø« Ø¬Ø¯ÙŠØ¯: Ø­Ø§ÙˆÙ„ Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ DESIRED_PER_PLATFORM
                links = duckduckgo_search_links(identifier, site=domain, num_results=DESIRED_PER_PLATFORM)

                if not links:
                    # ÙØ§Ù„Ù†Ù‡Ø§ÙŠØ© Ø¬Ø±Ø¨ Ø¨Ø­Ø« Ø¹Ø§Ù… Ø¨Ø¯ÙˆÙ† site: Ù„Ù„Ù‚Ø¨Ø¶ Ø¹Ù„Ù‰ Ø£ÙŠ Ø±Ø§Ø¨Ø· Ù…Ø±ØªØ¨Ø· Ø¨Ø§Ù„Ø§Ø³Ù…
                    links = duckduckgo_search_links(identifier, site=None, num_results=DESIRED_PER_PLATFORM)

                if not links:
                    print(f"âš ï¸ No links found for {platform_name} ({domain})", flush=True)

                for link in links:
                    results.append({"platform": platform_name, "link": link})
                    if client:
                        try:
                            client.execute(
                                "INSERT INTO search_cache (query, platform, link) VALUES (?, ?, ?)",
                                (identifier, platform_name, link)
                            )
                        except Exception as e:
                            print("Cache insert error:", e, flush=True)

                # ØªØ£Ø®ÙŠØ± Ø¨ÙŠÙ† Ø§Ù„Ù…Ù†ØµØ§Øª
                time.sleep(REQUEST_DELAY)

            except Exception as e:
                print(f"Unhandled error searching {platform_name}:", e, flush=True)
                traceback.print_exc()
                continue

        print(f"ğŸ”š Finished search for: {identifier} -> total results: {len(results)}", flush=True)
        return jsonify(results)

    except Exception as e:
        print("âŒ Fatal error in /search:", e, flush=True)
        traceback.print_exc()
        return jsonify([])
    

if __name__ == "__main__":
    # Ù„Ø§ ØªØºÙŠÙ‘Ø± port Ù‡Ù†Ø§ â€” Railway/hosting Ø³ÙŠØªØ­ÙƒÙ… Ø¨Ø§Ù„Ù€ port Ø¹Ø¨Ø± Ù…ØªØºÙŠØ± Ø§Ù„Ø¨ÙŠØ¦Ø© PORT
    port = int(os.getenv("PORT", 5000))
    app.run(host="0.0.0.0", port=port)
