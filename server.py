
# server.py
from flask import Flask, request, jsonify
from libsql_client import create_client_sync
from migrate import migrate
import os
import time
import requests
from bs4 import BeautifulSoup
import traceback
import re
import urllib.parse

app = Flask(__name__)

# ===== Ø¥Ø¹Ø¯Ø§Ø¯ Turso =====
DATABASE_URL = os.getenv("DATABASE_URL")
AUTH_TOKEN = os.getenv("AUTH_TOKEN")

client = None
try:
    client = create_client_sync(url=DATABASE_URL, auth_token=AUTH_TOKEN)
    migrate(client)
except Exception as e:
    print("âš ï¸ Turso init failed:", e)

# Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ø¬Ø¯Ø§ÙˆÙ„ Ø¥Ø°Ø§ Ø£Ù…ÙƒÙ†
if client:
    try:
        client.execute("""
        CREATE TABLE IF NOT EXISTS users_log (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            username TEXT,
            os TEXT,
            country TEXT,
            ip TEXT,
            search TEXT,
            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
        )
        """)
    except Exception as e:
        print("Error creating users_log:", e)

    try:
        client.execute("""
        CREATE TABLE IF NOT EXISTS search_cache (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            query TEXT,
            platform TEXT,
            link TEXT,
            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
        )
        """)
    except Exception as e:
        print("Error creating search_cache:", e)

# ===== Ø§Ù„Ù…Ù†ØµØ§Øª =====
PLATFORMS = {
    "Facebook": "facebook.com",
    "Instagram": "instagram.com",
    "Youtube": "youtube.com",
    "TikTok": "tiktok.com",
    "Snapchat": "snapchat.com",
    "Reddit": "reddit.com",
    "Twitter": "twitter.com",
    "Pinterest": "pinterest.com",
    "LinkedIn": "linkedin.com",
}

# Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ø´Ø¨ÙƒØ©
REQUEST_DELAY = float(os.getenv("REQUEST_DELAY", 1.5))  # ØªØ£Ø®ÙŠØ± Ø§ÙØªØ±Ø§Ø¶ÙŠ Ù„ØªÙ‚Ù„ÙŠÙ„ Ø§Ù„Ø¨Ù„ÙˆÙƒ
MAX_RETRIES = int(os.getenv("MAX_RETRIES", 2))

# Ø¬Ù„Ø³Ø© requests Ù…Ø¹ Header Ø«Ø§Ø¨Øª
session = requests.Session()
HEADERS = {
    "User-Agent": (
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
        "AppleWebKit/537.36 (KHTML, like Gecko) "
        "Chrome/120.0.0.0 Safari/537.36"
    )
}
session.headers.update(HEADERS)


def duckduckgo_search_links(query, site=None, num_results=10):
    """
    Ù†Ø­Ø§ÙˆÙ„ Ø¬Ù„Ø¨ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ù…Ù† DuckDuckGo HTML endpoint.
    - Ù†Ø³ØªØ®Ø¯Ù… GET Ù…Ø¹ params
    - Ù†ÙƒØ±Ø± Ù…Ø­Ø§ÙˆÙ„Ø§Øª (retries)
    - parse Ø¹Ø¨Ø± BeautifulSoup Ø£Ùˆ Ø¹Ø¨Ø± regex ÙƒÙØ§Ù„Ø¨Ø§Ùƒ
    - Ù†Ø¹ÙŠØ¯ Ø¯Ø§Ø¦Ù…Ø§ Ù‚Ø§Ø¦Ù…Ø© (Ø­ØªÙ‰ Ù„Ùˆ ÙØ§Ø±ØºØ©)
    """
    search_query = f"{query} site:{site}" if site else query
    url = "https://html.duckduckgo.com/html/"
    params = {"q": search_query}

    links = []

    for attempt in range(MAX_RETRIES + 1):
        try:
            resp = session.get(url, params=params, timeout=20)
            print(f"ğŸ” Searching: {search_query} | Status: {resp.status_code}")

            # 200 => Ø­Ø§ÙˆÙ„ ØªÙÙƒÙŠÙƒ Ø§Ù„Ù€ DOM
            if resp.status_code == 200:
                soup = BeautifulSoup(resp.text, "html.parser")

                # Ø£ÙˆÙ„ Ù…Ø­Ø§ÙˆÙ„Ø©: elements Ø¨Ø¹Ù„Ø§Ù…Ø© result__a
                anchors = soup.select("a.result__a")

                # Ø¥Ø°Ø§ Ù„Ù… Ù†Ø¬Ø¯ Ù‡Ø°Ø§ Ø§Ù„Ø´ÙƒÙ„ØŒ Ù†Ø£Ø®Ø° ÙƒÙ„ <a>
                if not anchors:
                    anchors = soup.find_all('a')

                # Ø­Ø§ÙˆÙ„ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø±Ø§Ø¨Ø· Ù…Ù† href Ø£Ùˆ Ù…Ù† Ø¯Ø§Ø®Ù„ Ø±Ø§Ø¨Ø· Ù…ØºÙ„Ù (uddg=...)
                for a in anchors:
                    link = None
                    href = a.get('href')
                    if href:
                        # Ø¨Ø¹Ø¶ Ø±ÙˆØ§Ø¨Ø· duckduckgo ØªØ£ØªÙŠ Ø¹Ù„Ù‰ Ø´ÙƒÙ„ /l/?kh=...&uddg=https%3A%2F%2F... => Ù†ÙÙƒ Ø§Ù„ØªØ´ÙÙŠØ±
                        if 'uddg=' in href:
                            m = re.search(r'uddg=([^&]+)', href)
                            if m:
                                try:
                                    link = urllib.parse.unquote(m.group(1))
                                except Exception:
                                    link = None
                        else:
                            link = href

                    # Ø¨Ø¹Ø¶ Ø§Ù„Ø±ÙˆØ§Ø¨Ø· Ù…Ø­ÙÙˆØ¸Ø© ÙÙŠ attribute Ø¢Ø®Ø±
                    if not link:
                        data_href = a.get('data-href') or a.get('data-redirect')
                        if data_href:
                            link = data_href

                    if link and link.startswith('http'):
                        if 'duckduckgo.com' not in link and link not in links:
                            links.append(link)
                    if len(links) >= num_results:
                        break

                # ÙØ§Ù„Ø¨Ø§Ùƒ: Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¹Ø¨Ø± regex Ù„Ùˆ Ù„Ù… Ù†Ø¬Ø¯ Ù†ØªØ§Ø¦Ø¬ ÙƒØ§ÙÙŠØ©
                if not links:
                    found = re.findall(r'href="(https?://[^"]+)"', resp.text)
                    for link in found:
                        if 'duckduckgo.com' not in link and link not in links:
                            links.append(link)
                        if len(links) >= num_results:
                            break

                return links

            # Ø­Ø§Ù„Ø§Øª Ù‚Ø§Ø¨Ù„Ø© Ù„Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø©
            elif resp.status_code in (429, 202):
                wait = 2 ** attempt
                print(f"â³ Got {resp.status_code}, retrying after {wait}s (attempt {attempt + 1})")
                time.sleep(wait)
                continue
            else:
                print("âŒ DuckDuckGo returned status:", resp.status_code)
                break

        except Exception as e:
            print("âš ï¸ Error searching:", e)
            traceback.print_exc()
            time.sleep(1)

    return links


@app.route("/search", methods=["POST"])
def search():
    try:
        data = request.get_json(silent=True) or {}
        identifier = (data.get("identifier", "") or "").strip()
        if not identifier:
            return jsonify([])

        # ØªØ®Ø²ÙŠÙ† Ø§Ù„Ø¨Ø­Ø« (Ù„ÙŠØ³ Ø³Ø¨Ø¨ 500 Ø¥Ø°Ø§ ÙØ´Ù„)
        if client:
            try:
                client.execute(
                    "INSERT INTO users_log (username, os, country, ip, search) VALUES (?, ?, ?, ?, ?)",
                    ("server_user", "ServerOS", "Unknown", "0.0.0.0", identifier)
                )
            except Exception as e:
                print("DB insert error:", e)

        results = []

        for platform_name, domain in PLATFORMS.items():
            try:
                # Ù…Ø­Ø§ÙˆÙ„Ø© Ø£Ø®Ø° Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ù…Ù† Ø§Ù„ÙƒØ§Ø´
                cached = []
                if client:
                    try:
                        res = client.execute(
                            "SELECT link FROM search_cache WHERE query=? AND platform=?",
                            (identifier, platform_name)
                        )
                        if hasattr(res, 'fetchall'):
                            cached = res.fetchall()
                        else:
                            # Ù‚Ø¯ ØªÙƒÙˆÙ† Ù‚Ø§Ø¦Ù…Ø© Ù…Ø¨Ø§Ø´Ø±Ø©
                            cached = list(res)
                    except Exception as e:
                        print("Cache select error:", e)
                        cached = []

                if cached:
                    for row in cached:
                        # row Ù‚Ø¯ ÙŠÙƒÙˆÙ† tuple Ø£Ùˆ Ø³Ø·Ø± ÙˆØ§Ø­Ø¯
                        link = row[0] if isinstance(row, (list, tuple)) else row
                        results.append({"platform": platform_name, "link": link})
                    continue

                # Ø¨Ø­Ø« Ø¬Ø¯ÙŠØ¯
                links = duckduckgo_search_links(identifier, domain, num_results=10)

                if not links:
                    print(f"âš ï¸ No links found for {platform_name} ({domain})")

                for link in links:
                    results.append({"platform": platform_name, "link": link})
                    if client:
                        try:
                            client.execute(
                                "INSERT INTO search_cache (query, platform, link) VALUES (?, ?, ?)",
                                (identifier, platform_name, link)
                            )
                        except Exception as e:
                            print("Cache insert error:", e)

                time.sleep(REQUEST_DELAY)

            except Exception as e:
                print(f"Unhandled error searching {platform_name}:", e)
                traceback.print_exc()
                # Ù„Ø§ Ù†ÙƒØ³Ø± Ø§Ù„Ø³ÙŠØ±ÙØ±ØŒ Ù†ÙƒÙ…Ù„ Ø§Ù„Ù…Ù†ØµØ§Øª Ø§Ù„Ø£Ø®Ø±Ù‰
                continue

        return jsonify(results)

    except Exception as e:
        print("âŒ Fatal error in /search:", e)
        traceback.print_exc()
        # Ù†Ø¹ÙŠØ¯ 200 Ù…Ø¹ Ù„Ø§Ø¦Ø­Ø© ÙØ§Ø±ØºØ© Ø¨Ø¯Ù„ 500 Ù„ÙƒÙŠ Ù„Ø§ ØªÙƒØ³Ø± Ø£Ø¯ÙˆØ§Øª Ø§Ù„Ø¹Ù…ÙŠÙ„
        return jsonify([])


if __name__ == "__main__":
    app.run(host="0.0.0.0", port=int(os.getenv("PORT", 5000)))
